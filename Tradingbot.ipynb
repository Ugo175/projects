{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1qBbmNI_3H1k80ZzZyUSWaV6j-a6SK8jG",
      "authorship_tag": "ABX9TyPRC7Id82pt2XlCxiqBktaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ugo175/projects/blob/main/Tradingbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/candlestick-patterns\n",
        "# Step 1: Clone the repository\n",
        "# Step 1: Install TA-Lib dependencies\n",
        "# !wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "# !tar -xzf ta-lib-0.4.0-src.tar.gz\n",
        "# !cd ta-lib && ./configure --prefix=/usr && make && make install\n",
        "\n",
        "# # Step 2: Install TA-Lib Python wrapper\n",
        "# !pip install ta-lib\n",
        "\n",
        "!git clone https://github.com/SpiralDevelopment/candlestick-patterns.git\n",
        "\n",
        "# Step 3: Add the cloned repo to the Python path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/candlestick-patterns/')\n",
        "sys.path.append('/content/candlestick-patterns/candlestick/patterns/')\n",
        "\n",
        "import pandas as pd\n",
        "from candlestick.patterns import *  # type: ignore\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "woC8BRq0dm-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6349491-4038-4005-c4f0-a24a5b0553ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'candlestick-patterns'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 79 (delta 6), reused 1 (delta 0), pack-reused 67 (from 1)\u001b[K\n",
            "Receiving objects: 100% (79/79), 18.87 KiB | 18.87 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/GBPUSD_Candlestick_1_Hour_BID_01.01.2022-10.11.2024.csv'  # Specify the correct file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Save the DataFrame to an XLSX file\n",
        "xlsx_file_path = '/content/drive/MyDrive/GBPUSD_Candlestick_1_Hour_BID_01.01.2022-10.11.2024.xlsx'  # Specify the path for the output XLSX file\n",
        "df.to_excel(xlsx_file_path, index=False)\n",
        "\n",
        "# Confirm the file is saved\n",
        "print(f\"File saved to {xlsx_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pX-8fcW2DGEK",
        "outputId": "01539f50-7fd6-4701-b79f-6dfb8de38f5e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to /content/drive/MyDrive/GBPUSD_Candlestick_1_Hour_BID_01.01.2022-10.11.2024.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/candlestick-patterns\n",
        "# # Step 1: Clone the repository\n",
        "# !git clone https://github.com/SpiralDevelopment/candlestick-patterns.git\n",
        "\n",
        "# Step 3: Add the cloned repo to the Python path\n",
        "import os\n",
        "import sys\n",
        "sys.path.extend([\n",
        "    '/content/candlestick-patterns',\n",
        "    '/content/candlestick-patterns/candlestick/patterns',\n",
        "])  # Adjust the path if necessary\n",
        "from inverted_hammer import *\n",
        "from hammer import *\n",
        "from hanging_man import *\n",
        "from bullish_engulfing import *\n",
        "from bearish_engulfing import *\n",
        "from bullish_harami import *\n",
        "from bearish_harami import *\n",
        "from doji import *\n",
        "from doji_star import *\n",
        "from dragonfly_doji import *\n",
        "from gravestone_doji import *\n",
        "from dark_cloud_cover import *\n",
        "from morning_star import *\n",
        "from morning_star_doji import *\n",
        "from piercing_pattern import *\n",
        "from rain_drop import *\n",
        "from rain_drop_doji import *\n",
        "from shooting_star import *\n",
        "\n",
        "# List all attributes in the patterns module\n",
        "print(dir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mM_8MvNL8o0",
        "outputId": "44423e2e-02b4-4c73-e5aa-29afa35eccdc",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BearishEngulfing', 'BearishHarami', 'BullishEngulfing', 'BullishHarami', 'CandlestickFinder', 'DarkCloudCover', 'Doji', 'DojiStar', 'DragonflyDoji', 'GravestoneDoji', 'Hammer', 'HangingMan', 'In', 'InvertedHammer', 'MorningStar', 'MorningStarDoji', 'Out', 'PiercingPattern', 'RainDrop', 'RainDropDoji', 'ShootingStar', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_exit_code', '_i', '_i1', '_i2', '_i3', '_i4', '_i5', '_ih', '_ii', '_iii', '_oh', 'csv_file_path', 'df', 'exit', 'get_ipython', 'os', 'pd', 'quit', 'sys', 'xlsx_file_path']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the EURUSD dataset\n",
        "file_path = '/content/drive/MyDrive/GBPUSD_Candlestick_1_Hour_BID_01.01.2022-10.11.2024.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Rename columns if needed\n",
        "df.columns = ['Local_time', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "cols = ['Open', 'High', 'Low', 'Close']\n",
        "print(df.head())\n",
        "\n",
        "# Detect candlestick patterns and create True/False columns for each\n",
        "# Specify 'Close' column as the target for pattern detection\n",
        "candles_args = {\"candles_df\": df, \"ohlc\": cols, \"is_reversed\": False}\n",
        "df['Inverted_Hammer'] = InvertedHammer(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Hammer'] = Hammer(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Hanging_Man'] = HangingMan(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Bullish_Engulfing'] = BullishEngulfing(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Bearish_Engulfing'] = BearishEngulfing(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Bullish_Harami'] = BullishHarami(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Bearish_Harami'] = BearishHarami(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Doji'] = Doji(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Doji_Star'] = DojiStar(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Dragonfly_Doji'] = DragonflyDoji(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Gravestone_Doji'] = GravestoneDoji(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Dark_Cloud_Cover'] = DarkCloudCover(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Morning_Star'] = MorningStar(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Morning_Star_Doji'] = MorningStarDoji(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Piercing_Pattern'] = PiercingPattern(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Rain_Drop'] = RainDrop(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Rain_Drop_Doji'] = RainDropDoji(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "df['Shooting_Star'] = ShootingStar(target='Temp').has_pattern(**candles_args)['Temp']\n",
        "\n",
        "# Save the results\n",
        "df.to_csv('GBPUSD_patterns_output.csv', index=False)\n",
        "\n",
        "print(\"Candlestick pattern detection completed and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ySDjxita7rN",
        "outputId": "4405cfbd-5c65-4566-db1b-c0713e4da28c",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         Local_time     Open     High      Low    Close  \\\n",
            "0  01.01.2022 00:00:00.000 GMT-0600  1.35177  1.35177  1.35177  1.35177   \n",
            "1  01.01.2022 01:00:00.000 GMT-0600  1.35177  1.35177  1.35177  1.35177   \n",
            "2  01.01.2022 02:00:00.000 GMT-0600  1.35177  1.35177  1.35177  1.35177   \n",
            "3  01.01.2022 03:00:00.000 GMT-0600  1.35177  1.35177  1.35177  1.35177   \n",
            "4  01.01.2022 04:00:00.000 GMT-0600  1.35177  1.35177  1.35177  1.35177   \n",
            "\n",
            "   Volume  \n",
            "0     0.0  \n",
            "1     0.0  \n",
            "2     0.0  \n",
            "3     0.0  \n",
            "4     0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/candlestick-patterns/candlestick/patterns/doji.py:17: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return abs(close - open) / (high - low) < 0.1 and \\\n",
            "/content/candlestick-patterns/candlestick/patterns/doji_star.py:25: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  abs(close - open) / (high - low) < 0.1 and \\\n",
            "/content/candlestick-patterns/candlestick/patterns/dragonfly_doji.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return abs(close - open) / (high - low) < 0.1 and \\\n",
            "/content/candlestick-patterns/candlestick/patterns/gravestone_doji.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return (abs(close - open) / (high - low) < 0.1 and\n",
            "/content/candlestick-patterns/candlestick/patterns/morning_star_doji.py:30: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  abs(prev_close - prev_open) / (prev_high - prev_low) < 0.1 and\n",
            "/content/candlestick-patterns/candlestick/patterns/rain_drop.py:24: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  0.3 > abs(close - open) / (high - low) >= 0.1 and\n",
            "/content/candlestick-patterns/candlestick/patterns/rain_drop_doji.py:24: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  abs(close - open) / (high - low) < 0.1 and\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candlestick pattern detection completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import talib as ta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and Prepare the Data\n",
        "data = pd.read_csv('GBPUSD_patterns_output.csv')\n",
        "\n",
        "# Define pattern columns and initial feature columns\n",
        "pattern_columns = (['Bearish_Engulfing', 'Bearish_Harami', 'Bullish_Engulfing', 'Bullish_Harami',\n",
        "                    'Dark_Cloud_Cover', 'Doji', 'Doji_Star', 'Dragonfly_Doji', 'Gravestone_Doji', 'Hammer', 'Hanging_Man',\n",
        "                    'Inverted_Hammer', 'Morning_Star', 'Morning_Star_Doji', 'Piercing_Pattern', 'Rain_Drop', 'Rain_Drop_Doji', 'Shooting_Star'])\n",
        "feature_columns = ['Open', 'High', 'Low', 'Close']  # Basic OHLC columns\n",
        "\n",
        "# Duplicate rows with rare patterns\n",
        "balanced_data = data.copy()  # Start with a copy of the original data\n",
        "\n",
        "for col in pattern_columns:\n",
        "    positive_samples = data[data[col] == 1]  # Get all rows where the pattern is present\n",
        "    balanced_data = pd.concat([balanced_data, positive_samples], ignore_index=True)  # Duplicate only for the current pattern\n",
        "\n",
        "# After duplicating, fill NaNs and convert to binary as needed\n",
        "for col in pattern_columns:\n",
        "    balanced_data[col] = balanced_data[col].fillna(0).astype(int)\n",
        "\n",
        "\n",
        "# Step 1b: Calculate Technical Indicators and Add as Features\n",
        "data['SMA_50'] = ta.SMA(data['Close'], timeperiod=50)\n",
        "data['EMA_200'] = ta.EMA(data['Close'], timeperiod=200)\n",
        "data['RSI_14'] = ta.RSI(data['Close'], timeperiod=14)\n",
        "data['MACD'], data['MACD_signal'], data['MACD_hist'] = ta.MACD(data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
        "data['upper_band'], data['middle_band'], data['lower_band'] = ta.BBANDS(data['Close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
        "data['ATR_14'] = ta.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "data['Stochastic_Oscillator'] = ta.STOCH(data['High'], data['Low'], data['Close'])[0]\n",
        "data['Williams_R'] = ta.WILLR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "data['CCI_20'] = ta.CCI(data['High'], data['Low'], data['Close'], timeperiod=20)\n",
        "\n",
        "# Drop NaNs resulting from indicator calculations\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Update feature columns to include technical indicators\n",
        "feature_columns += ['SMA_50', 'EMA_200', 'RSI_14', 'MACD', 'MACD_signal', 'MACD_hist', 'upper_band',\n",
        "                    'middle_band', 'lower_band', 'ATR_14', 'Stochastic_Oscillator', 'Williams_R', 'CCI_20']\n",
        "\n",
        "# Step 2: Create Sequences for LSTM\n",
        "def create_sequences(data, feature_columns, target_columns, time_steps=10):\n",
        "    X, y = [], {col: [] for col in target_columns}\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data[feature_columns].iloc[i:i+time_steps].values)  # Collect feature sequence\n",
        "        for col in target_columns:\n",
        "            y[col].append(data[col].iloc[i + time_steps])  # Collect target value at end of sequence\n",
        "    X = np.array(X)\n",
        "    y = np.column_stack([np.array(y[col]) for col in target_columns])  # Stack targets into a multi-output array\n",
        "    return X, y\n",
        "\n",
        "# Create sequences with the specified time window\n",
        "time_steps = 10\n",
        "X, y = create_sequences(data, feature_columns, pattern_columns, time_steps=time_steps)\n",
        "\n",
        "# Step 3: Split Data into Train and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Define the CNN-LSTM Model with Bidirectional LSTM and Additional Dense Layers\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.15))\n",
        "\n",
        "# Add additional Dense layers for complexity\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Final output layer with one neuron per pattern and sigmoid activation\n",
        "model.add(Dense(len(pattern_columns), activation='sigmoid'))\n",
        "\n",
        "# Step 5: Compile the Model with a Lower Learning Rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 6: Train the Model with Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "# Step 7: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Step 8: Make Predictions\n",
        "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")  # Convert probabilities to binary values (1/0)\n",
        "print(predictions[:5])  # Display first 5 predictions for each pattern\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UqwVWbmFudsy",
        "outputId": "b3644361-46aa-4a2b-8487-95482018c205"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.1509 - loss: 0.1668 - val_accuracy: 0.1240 - val_loss: 0.0639\n",
            "Epoch 2/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.1787 - loss: 0.0673 - val_accuracy: 0.1394 - val_loss: 0.0588\n",
            "Epoch 3/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.2187 - loss: 0.0616 - val_accuracy: 0.1926 - val_loss: 0.0579\n",
            "Epoch 4/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2185 - loss: 0.0594 - val_accuracy: 0.1939 - val_loss: 0.0557\n",
            "Epoch 5/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2182 - loss: 0.0571 - val_accuracy: 0.2292 - val_loss: 0.0555\n",
            "Epoch 6/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2151 - loss: 0.0572 - val_accuracy: 0.2672 - val_loss: 0.0552\n",
            "Epoch 7/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2266 - loss: 0.0571 - val_accuracy: 0.2469 - val_loss: 0.0550\n",
            "Epoch 8/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2285 - loss: 0.0553 - val_accuracy: 0.2507 - val_loss: 0.0548\n",
            "Epoch 9/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2380 - loss: 0.0558 - val_accuracy: 0.2702 - val_loss: 0.0549\n",
            "Epoch 10/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2621 - loss: 0.0550 - val_accuracy: 0.2869 - val_loss: 0.0546\n",
            "Epoch 11/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2567 - loss: 0.0552 - val_accuracy: 0.2897 - val_loss: 0.0543\n",
            "Epoch 12/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2630 - loss: 0.0541 - val_accuracy: 0.2459 - val_loss: 0.0541\n",
            "Epoch 13/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2488 - loss: 0.0546 - val_accuracy: 0.2910 - val_loss: 0.0539\n",
            "Epoch 14/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2563 - loss: 0.0550 - val_accuracy: 0.2801 - val_loss: 0.0540\n",
            "Epoch 15/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.2569 - loss: 0.0544 - val_accuracy: 0.2847 - val_loss: 0.0538\n",
            "Epoch 16/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.2667 - loss: 0.0539 - val_accuracy: 0.2989 - val_loss: 0.0548\n",
            "Epoch 17/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2596 - loss: 0.0541 - val_accuracy: 0.2697 - val_loss: 0.0537\n",
            "Epoch 18/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2606 - loss: 0.0530 - val_accuracy: 0.3120 - val_loss: 0.0542\n",
            "Epoch 19/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2769 - loss: 0.0538 - val_accuracy: 0.2849 - val_loss: 0.0540\n",
            "Epoch 20/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2804 - loss: 0.0517 - val_accuracy: 0.2768 - val_loss: 0.0537\n",
            "Epoch 21/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.2663 - loss: 0.0534 - val_accuracy: 0.2489 - val_loss: 0.0541\n",
            "Epoch 22/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2775 - loss: 0.0518 - val_accuracy: 0.2707 - val_loss: 0.0540\n",
            "Epoch 23/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.2698 - loss: 0.0530 - val_accuracy: 0.2783 - val_loss: 0.0545\n",
            "Epoch 24/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2744 - loss: 0.0523 - val_accuracy: 0.2672 - val_loss: 0.0540\n",
            "Epoch 25/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2713 - loss: 0.0516 - val_accuracy: 0.2875 - val_loss: 0.0540\n",
            "Epoch 26/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.2901 - loss: 0.0520 - val_accuracy: 0.2431 - val_loss: 0.0543\n",
            "Epoch 27/50\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2736 - loss: 0.0512 - val_accuracy: 0.2608 - val_loss: 0.0544\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2587 - loss: 0.0580\n",
            "Test Accuracy: 0.25\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}